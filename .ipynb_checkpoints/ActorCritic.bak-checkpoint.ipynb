{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTZOxYE2Yf4Y"
   },
   "source": [
    "### An implementation of an actor critic RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qwyeeufTY8mU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rw8ITgIyYlGq"
   },
   "outputs": [],
   "source": [
    "# DOES NOT USE EXPERIENCE REPLAY.\n",
    "# I assume that it is bad for quantum. Remember to ask.\n",
    "class ActorCritic():\n",
    "  \"\"\"A class encapsulating a hybrid actor critic network.\"\"\"\n",
    "  def __init__(self, actor_learning_rate,\n",
    "               critic_learning_rate,\n",
    "               regularisation,\n",
    "               discount_factor, num_states,\n",
    "               num_actions, network_dimensions,\n",
    "               activation):\n",
    "    self.gamma = discount_factor\n",
    "    self.num_actions = num_actions\n",
    "    self.memory = []\n",
    "    self.actor, self.critic = self.create_model((actor_learning_rate, critic_learning_rate),\n",
    "                                                regularisation,\n",
    "                                                num_states, num_actions,\n",
    "                                                network_dimensions, activation)\n",
    "\n",
    "  def act(self, observation):\n",
    "    return np.random.choice(self.num_actions,\n",
    "                            p=self.actor.predict(observation[np.newaxis, :],\n",
    "                                                 verbose=False).flatten())\n",
    "  def save(self, timestamp):\n",
    "      self.actor.save(f\"actor_{timestamp}.keras\")\n",
    "      \n",
    "\n",
    "  # Could use experience replay here.\n",
    "  def learn(self, observation, action, reward, new_observation, done):\n",
    "    self.memory.append((observation, action, reward, new_observation, done))\n",
    "\n",
    "    if not done:\n",
    "      return\n",
    "\n",
    "    states, actions, rewards, _, _ = zip(*self.memory)\n",
    "    states = np.vstack(states)\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "      #     rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "      # discounted_sum = tf.constant(0.0)\n",
    "      # discounted_sum_shape = discounted_sum.shape\n",
    "      # for i in tf.range(n):\n",
    "      #   reward = rewards[i]\n",
    "      #   discounted_sum = reward + gamma * discounted_sum\n",
    "      #   discounted_sum.set_shape(discounted_sum_shape)\n",
    "      #   returns = returns.write(i, discounted_sum)\n",
    "      # returns = returns.stack()[::-1]\n",
    "\n",
    "\n",
    "    # Calculate Expected Return:\n",
    "    returns = np.zeros_like(rewards)\n",
    "    discounted_sum = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + self.gamma * discounted_sum\n",
    "        discounted_sum = returns[i]\n",
    "    \n",
    "    returns = (returns - np.mean(returns)) / (np.std(returns) + np.finfo(np.float32).eps.item())\n",
    "    #print(returns)\n",
    "      \n",
    "    # Update the Critic:\n",
    "    critic_rewards = self.critic.predict(states, verbose=False).flatten()\n",
    "    history = self.critic.fit(states, returns, verbose=False, batch_size=len(self.memory))\n",
    "    #print('Critic Loss: ', history.history['loss'])\n",
    "\n",
    "    # Calculate Advantage:\n",
    "    advantages = returns - critic_rewards\n",
    "\n",
    "    # Update the Actor:\n",
    "    actions_one_hot = np.zeros((len(actions), self.num_actions))\n",
    "    actions_one_hot[np.arange(len(actions)), actions] = 1\n",
    "    #target_actions_and_advantage = np.hstack([actions_one_hot, advantages[:, np.newaxis]])\n",
    "    self.actor.fit(states, actions_one_hot, sample_weight=advantages, verbose=False, batch_size=len(self.memory))\n",
    "\n",
    "    self.memory = []\n",
    "\n",
    "  def create_model(self, alpha, regularisation, num_states, num_actions,\n",
    "                   network_dimensions, activation):\n",
    "    \"\"\"Creates the actor and critic models from the network dimensions\"\"\"\n",
    "    # This is a hybrid network, where the actor and critic are just\n",
    "    # different outputs of the same net.\n",
    "    # Model Inputs:\n",
    "    inputs = keras.Input(shape=(num_states,))\n",
    "    #advantage = keras.Input(shape=1) # Dummy input used to calculate the loss for the actor.\n",
    "\n",
    "    # Model Structure:\n",
    "    net = inputs\n",
    "    for i, dim in enumerate(network_dimensions):\n",
    "        net = keras.layers.Dense(dim, activation=activation, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularisation))(net)\n",
    "\n",
    "    # Model Outputs:\n",
    "    action_probabilities = keras.layers.Dense(num_actions, activation='softmax')(net)\n",
    "    critic_reward = keras.layers.Dense(1, activation='linear')(net)\n",
    "\n",
    "    # Model Creation:\n",
    "    actor = keras.Model(inputs=[inputs], outputs=[action_probabilities])\n",
    "    critic = keras.Model(inputs=[inputs], outputs=[critic_reward])\n",
    "\n",
    "    # Actor Loss:\n",
    "    # @tf.function()\n",
    "    # def actor_loss(target_actions_and_advantage, predicted_actions):\n",
    "    #   # The advantage is passed to the actor as one of the target values,\n",
    "    #   # just to get it inside the loss function, as this more elegant than\n",
    "    #   # the gradient tape stuff.\n",
    "    #   target_actions = tf.cast(target_actions_and_advantage[:, :num_actions], tf.float32)\n",
    "    #   advantage = tf.cast(target_actions_and_advantage[:, num_actions], tf.float32)\n",
    "    #   log_likelihood = tf.reduce_sum(target_actions * -tf.math.log(predicted_actions + 1e-10), axis=1)\n",
    "    #   loss = tf.reduce_mean(log_likelihood * advantage)\n",
    "    #   \"\"\"tf.print('Actor Loss: ', loss)\n",
    "    #   tf.print('Target Actions: ', target_actions)\n",
    "    #   tf.print('Predicted Actions: ', predicted_actions)\n",
    "    #   tf.print('Log Predicted Actions: ', tf.math.log(tf.clip_by_value(predicted_actions, 1e-6, np.inf)))\n",
    "    #   tf.print('Log Likelihood: ', log_likelihood)\"\"\"\n",
    "    #   #tf.print('Advantage: ', advantage)\n",
    "    #   #tf.print('Actor Loss: ', loss)\n",
    "    #   return loss\n",
    "\n",
    "    actor.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha[0]), loss=\"categorical_crossentropy\")\n",
    "    critic.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha[1]), loss='huber')\n",
    "\n",
    "    return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FQG_GxvdvSs0",
    "outputId": "9461b522-b48e-43f0-8fab-836e23469cc2"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m position \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m4.8\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(position)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4.8\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m12\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m12\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(angle))\n\u001b[0;32m---> 33\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn(observation, action, reward, new_observation, done \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m max_steps)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Update:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m, in \u001b[0;36mActorCritic.learn\u001b[0;34m(self, observation, action, reward, new_observation, done)\u001b[0m\n\u001b[1;32m     70\u001b[0m actions_one_hot[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(actions)), actions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#target_actions_and_advantage = np.hstack([actions_one_hot, advantages[:, np.newaxis]])\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mfit(states, actions_one_hot, sample_weight\u001b[38;5;241m=\u001b[39madvantages[\u001b[38;5;241m0\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/QRL/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/QRL/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m (sample_weights, _, _) \u001b[38;5;241m=\u001b[39m training_utils\u001b[38;5;241m.\u001b[39mhandle_partial_sample_weights(\n\u001b[1;32m    260\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    263\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m    265\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28mint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[1;32m    267\u001b[0m )\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Set the seed.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = ActorCritic(5e-3, 5e-3, 1e-7, 0.9, env.observation_space.shape[0], env.action_space.n, [128], 'relu')\n",
    "#agent = ActorCritic(1e-2, 1e-2, 0, 0.9, 4, 2, [128], 'relu')\n",
    "# get average reward during training episodes and plot.\n",
    "total_rewards = []\n",
    "average_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "max_steps = 500\n",
    "max_episodes = 5000\n",
    "episode_average = 50\n",
    "episode_threshold = 480\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  total_reward = 0.0\n",
    "  step = 0\n",
    "  while step != max_steps and not done:\n",
    "    # Calculate action\n",
    "    action = agent.act(observation)\n",
    "\n",
    "    # step forward\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    angle = observation[2]\n",
    "    position = observation[0]\n",
    "    reward = 0.1 * (4.8 - np.abs(position)) / 4.8 + 0.9 * (12/np.pi) * (np.pi/12 - np.abs(angle))\n",
    "    agent.learn(observation, action, reward, new_observation, done or step == max_steps)\n",
    "\n",
    "    # Update:\n",
    "    total_reward += reward\n",
    "    observation = new_observation\n",
    "    step += 1\n",
    "\n",
    "  agent.memory = []\n",
    "  total_rewards.append(total_reward)\n",
    "  average_rewards.append(total_reward / step)\n",
    "  episode_lengths.append(step)\n",
    "\n",
    "  average_episode_length = np.mean(episode_lengths[-episode_average:])\n",
    "\n",
    "  if len(episode_lengths) > episode_average and average_episode_length > episode_threshold:\n",
    "    print(f\"\\nSolved at Episode {episode}, Average Episode Length: {average_episode_length:.1f}\")\n",
    "    break\n",
    "  else:\n",
    "    print(f\"\\rEpisode: {episode}, Steps: {step: >3}, Average Episode Length: {average_episode_length: >4.1f}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "jvFFGJKlwm63",
    "outputId": "f6fb4785-267f-43c6-d1c9-a1ee5aba8af3"
   },
   "outputs": [],
   "source": [
    "# ts = '2024.07.04-17.27'\n",
    "# results = np.load(f'results_{ts}.npy', allow_pickle=True)\n",
    "# total_rewards = results.item().get(\"Total Reward\")\n",
    "# average_rewards = results.item().get(\"Average Reward\")\n",
    "# episode_lengths = results.item().get(\"Episode Length\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "ax[0][0].set_xlabel('Episode')\n",
    "ax[0][0].set_ylabel('Total Reward')\n",
    "ax[0][0].plot(total_rewards)\n",
    "# total_rewards_rolling = np.convolve(total_rewards, np.ones(len(total_rewards)//10), 'same') / (len(total_rewards)//10)\n",
    "# ax[0][0].plot(total_rewards_rolling)\n",
    "\n",
    "ax[0][1].set_xlabel('Episode')\n",
    "ax[0][1].set_ylabel('Average Reward')\n",
    "ax[0][1].plot(average_rewards)\n",
    "# average_rewards_rolling = np.convolve(average_rewards, np.ones(len(average_rewards)//10), 'same') / (len(average_rewards)//10)\n",
    "# ax[0][1].plot(average_rewards_rolling)\n",
    "\n",
    "ax[1][0].set_xlabel('Episode')\n",
    "ax[1][0].set_ylabel('Cumulative Reward')\n",
    "cumulative_rewards = np.cumsum(total_rewards)\n",
    "ax[1][0].plot(cumulative_rewards)\n",
    "\n",
    "ax[1][1].set_xlabel('Episode')\n",
    "ax[1][1].set_ylabel('Episode Length')\n",
    "ax[1][1].plot(episode_lengths, label='Lengths')\n",
    "# episode_lengths_rolling = np.convolve(episode_lengths, np.ones(len(episode_lengths)//10), 'same') / (len(episode_lengths)//10)\n",
    "# ax[1][1].plot(episode_lengths_rolling, 'b--', label='Smoothed Lengths')\n",
    "# ax[1][1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.strftime(\"%Y.%m.%d-%H.%M\")\n",
    "#np.save(f'total_reward_{timestamp}.npy', total_rewards)\n",
    "#np.save(f'running_reward_{timestamp}.npy', running_rewards)\n",
    "#np.save(f'average_reward_{timestamp}.npy', average_rewards)\n",
    "results = {\"Total Reward\":total_rewards,\n",
    "           \"Cumulative Reward\": np.cumsum(total_rewards),\n",
    "           \"Average Reward\": average_rewards,\n",
    "           \"Episode Length\": episode_lengths}\n",
    "np.save(f'results_{timestamp}.npy', results)\n",
    "agent.save(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play:\n",
    "import matplotlib.animation as anim\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "\n",
    "actor = keras.models.load_model(f\"actor_{timestamp}.keras\", compile=False)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "frames=[]\n",
    "for i in range(500):\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "  action = np.random.choice(env.action_space.n,\n",
    "                                p=actor.predict(observation[np.newaxis, :],\n",
    "                                                 verbose=False).flatten())\n",
    "  observation, _, done, _ = env.step(action)\n",
    "  if done:\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "def init():\n",
    "    global im, text\n",
    "    im = ax.imshow(frames[0])\n",
    "    text = ax.text(.05, .95, '1', ha='left',\n",
    "                   va='top', transform=ax.transAxes) \n",
    "\n",
    "def animate(i):\n",
    "  global im, text\n",
    "  im.set_data(frames[i])\n",
    "  text.set_text(str(i))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.axis('off')\n",
    "\n",
    "animation = anim.FuncAnimation(fig, animate, init_func=init, frames=500)\n",
    "animation.save(f'animation_{timestamp}.gif', writer='ffmpeg', fps=30)\n",
    "plt.close(fig)\n",
    "embed.embed_file(f'animation_{timestamp}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
