{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694010ec-11f9-4560-a8af-9811a11a182b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8565d7-2922-4d57-a726-40a2edcbb210",
   "metadata": {},
   "source": [
    "### A quantum implementation of an actor critic RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1773330-9590-4ac5-9908-1e13b9d45fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "from tensorflow import keras\n",
    "import gym\n",
    "import cirq\n",
    "import sympy\n",
    "\n",
    "\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "from functools import reduce\n",
    "from collections import deque, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4059e-3123-48cb-b3c0-33dcc68b6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Circuit(tf.keras.layers.Layer):\n",
    "    \"\"\"Implements a parameterised quantum circuit with input re-uploading as a layer.\"\"\"\n",
    "    def __init__(self, num_layers, name=\"PQC\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_qubits = input_shape[0][1]\n",
    "        self.qubits = cirq.GridQubit.rect(1, self.num_qubits)\n",
    "        \n",
    "        # Symbols for PQC weights\n",
    "        params = sympy.symbols(f'param(0:{3 * self.num_layers * self.num_qubits})') # three rotations per layer per qubit\n",
    "        inputs = sympy.symbols(f'input(0:{self.num_layers})_(0:{self.num_qubits})')\n",
    "        params = np.asarray(params).reshape((self.num_layers, self.num_qubits, 3))\n",
    "        inputs = np.asarray(inputs).reshape((self.num_layers, self.num_qubits))\n",
    "\n",
    "        # Circuit Definition\n",
    "        circuit = cirq.Circuit()\n",
    "        for l in range(self.num_layers):\n",
    "            circuit += self.encode(self.qubits, inputs[l])\n",
    "            circuit += self.entangle(self.qubits)\n",
    "            circuit += self.rotate(self.qubits, params[l])\n",
    "\n",
    "        self.rotation_weights = self.add_weight(shape=(3 * self.num_qubits * self.num_layers,),\n",
    "                                     initializer='random_normal',\n",
    "                                     trainable=True, name=\"Rotation_Weights\")\n",
    "        \n",
    "        self.input_weights = self.add_weight(shape=(self.num_qubits * self.num_layers,),\n",
    "                                     initializer='ones',\n",
    "                                     trainable=True, name=\"Input_Weights\")\n",
    "        \n",
    "        # Symbol order\n",
    "        symbols = list(map(str, np.append(params.flatten(), inputs.flatten())))\n",
    "        self.symbol_order = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "        self.pqc = tfq.layers.ControlledPQC(circuit, [cirq.Z(q) for q in self.qubits])\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super().get_config().copy()\n",
    "        conf.update({\n",
    "            'num_layers': self.n_layers,\n",
    "            'rotation_weights': self.rotation_weights,\n",
    "            'input_weights': self.input_weights,\n",
    "            'name': self.name,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    @staticmethod\n",
    "    def encode(qubits, inputs):\n",
    "        \"\"\"Returns a layer encoding the state\"\"\"\n",
    "        return cirq.Circuit(cirq.rx(inputs[i])(q) for i, q in enumerate(qubits))\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate(qubits, params):\n",
    "        \"\"\"Returns a layer rotating each qubit.\"\"\"\n",
    "        return cirq.Circuit([cirq.rx(params[i, 0])(q), cirq.ry(params[i, 1])(q), cirq.rz(params[i, 2])(q)] for i, q in enumerate(qubits))\n",
    "\n",
    "    @staticmethod\n",
    "    def entangle(qubits):\n",
    "        \"\"\"Returns a layer entangling the qubits with CZ gates.\"\"\"\n",
    "        return [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, np.roll(qubits, -1, axis=0))]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # #batch_size = tf.shape(inputs[0])[0]\n",
    "        # batch_size = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        \n",
    "        # tiled_params = tf.tile([self.rotation_weights], multiples=[batch_size, 1])\n",
    "        # tiled_input_weights = tf.tile([self.input_weights], multiples=[batch_size, 1])\n",
    "\n",
    "        # tiled_inputs = tf.repeat(inputs[0], repeats=self.num_layers, axis=1)\n",
    "        # # tiled_inputs = tf.tile(tiled_inputs, multiples=[batch_size, 1])\n",
    "        # # tf.print(tiled_inputs.shape)\n",
    "\n",
    "\n",
    "        # tiled_weighted_inputs = tf.multiply(tiled_input_weights, tiled_inputs)\n",
    "\n",
    "        # parameters = tf.concat([tiled_params, tiled_weighted_inputs], axis=1)\n",
    "        # parameters_ordered = tf.gather(\n",
    "        #                 parameters,\n",
    "        #                 self.symbol_order, axis=1)\n",
    "        \n",
    "        # circuits = tf.repeat(tfq.convert_to_tensor([cirq.Circuit()]),\n",
    "        #                 repeats=batch_size)\n",
    "        \n",
    "        # return self.pqc([circuits, parameters_ordered])\n",
    "                #batch_size = tf.shape(inputs[0])[0]\n",
    "        batch_size = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        \n",
    "        tiled_params = tf.tile([self.rotation_weights], multiples=[batch_size, 1])\n",
    "\n",
    "        #input = tf.repeat(inputs[0], repeats=self.num_layers, axis=1)\n",
    "\n",
    "        repeated_inputs = tf.repeat(inputs[0], repeats=self.num_layers, axis=1)\n",
    "        weighted_inputs = tf.multiply(repeated_inputs, self.input_weights)\n",
    "        tiled_weighted_inputs = tf.tile(weighted_inputs, multiples=[1, batch_size])\n",
    "\n",
    "        parameters = tf.concat([tiled_params, tiled_weighted_inputs], axis=1)\n",
    "        parameters_ordered = tf.gather(\n",
    "                        parameters,\n",
    "                        self.symbol_order, axis=1)\n",
    "        \n",
    "        circuits = tf.repeat(tfq.convert_to_tensor([cirq.Circuit()]),\n",
    "                        repeats=batch_size)\n",
    "        \n",
    "        return self.pqc([circuits, parameters_ordered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73b12e-5712-4500-8c7b-120320f0d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=\"Scaling\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=input_shape[0], initializer='ones',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.multiply(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1e68f-3e1a-47d7-b1d4-57181e2828b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "  \"\"\"A class encapsulating a hybrid actor critic network.\"\"\"\n",
    "  def __init__(self, actor_learning_rate,\n",
    "               critic_learning_rate,\n",
    "               discount_factor, num_states,\n",
    "               num_actions, num_layers):\n",
    "    #self.observables = [reduce((lambda x, y: x * y), [cirq.Z(q) for q in qubits])] # Z_0*Z_1*Z_2*Z_3\n",
    "    self.gamma = discount_factor\n",
    "    self.num_actions = num_actions\n",
    "    self.memory = []\n",
    "    self.actor, self.critic, self.model = self.create_model((actor_learning_rate, critic_learning_rate),\n",
    "                                                num_states, num_actions,\n",
    "                                                num_layers)\n",
    "\n",
    "  def act(self, observation):\n",
    "    #print(self.actor.predict(observation[np.newaxis, :], verbose=False).flatten())\n",
    "    return np.random.choice(self.num_actions,\n",
    "                            p=self.actor.predict(observation[np.newaxis, :],\n",
    "                                                 verbose=False).flatten())\n",
    "  def save(self, timestamp):\n",
    "      self.actor.save(f\"actor_{timestamp}.keras\")\n",
    "      \n",
    "  # Could use experience replay here.\n",
    "  # Use tensorflow graphs to optimise training and results\n",
    "  # split into record and learn. just figure out tf graphs.\n",
    "  #@tf.function\n",
    "  def learn(self, observation, action, reward, new_observation, done):\n",
    "    self.memory.append((observation, action, reward, new_observation, done))\n",
    "\n",
    "    if not done:\n",
    "      return\n",
    "\n",
    "    states, actions, rewards, _, _ = zip(*self.memory)\n",
    "    states = np.vstack(states)\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    # Calculate Expected Return:\n",
    "    returns = np.zeros_like(rewards)\n",
    "    discounted_sum = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + self.gamma * discounted_sum\n",
    "        discounted_sum = returns[i]\n",
    "    \n",
    "    returns = (returns - np.mean(returns)) / np.std(returns)\n",
    "    #print(returns)\n",
    "      \n",
    "    # Update the Critic:\n",
    "    critic_rewards = self.critic.predict(states, verbose=False).flatten()\n",
    "    self.critic.fit(states, returns, verbose=False, batch_size=len(self.memory))\n",
    "    #print(critic_rewards)\n",
    "      \n",
    "    # Calculate Advantage:\n",
    "    advantages = (returns - critic_rewards)\n",
    "\n",
    "    # Update the Actor:\n",
    "    actions_one_hot = np.zeros((len(actions), self.num_actions))\n",
    "    actions_one_hot[np.arange(len(actions)), actions] = 1\n",
    "    #target_actions_and_advantage = np.hstack([actions_one_hot, advantages[:, np.newaxis]])\n",
    "    self.actor.fit(states, actions_one_hot, sample_weight=advantages, verbose=False, batch_size=len(self.memory))\n",
    "\n",
    "    self.memory = []\n",
    "\n",
    "  def create_model(self, alpha, num_states, num_actions, num_layers):\n",
    "    \"\"\"Creates the actor and critic models from the network dimensions\"\"\"\n",
    "    input_tensor = tf.keras.Input(shape=num_states, dtype=tf.dtypes.float32, name='Input')\n",
    "    re_uploading_pqc = Circuit(num_layers)([input_tensor])\n",
    "    scaling_layer = Scale()(re_uploading_pqc)\n",
    "\n",
    "    policy_logits = tf.keras.layers.Lambda(lambda x: x[:, :2], name=\"Policy-Split\")(scaling_layer)  # First two for policy\n",
    "    value_output = tf.keras.layers.Lambda(lambda x: x[:, 2] * x[:, 3], name=\"Value-Product\")(scaling_layer)\n",
    "\n",
    "    policy_output = tf.keras.layers.Softmax()(policy_logits)\n",
    "    actor = tf.keras.Model(inputs=[input_tensor], outputs=policy_output)\n",
    "    critic = tf.keras.Model(inputs=[input_tensor], outputs=value_output)\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=[policy_output, value_output])\n",
    "\n",
    "    actor.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha[0]), loss=\"categorical_crossentropy\")\n",
    "    critic.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha[1]), loss='huber')\n",
    "\n",
    "    return actor, critic, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957f9e3-df19-4cf4-92c0-54beac0d60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "#tf.debugging.disable_traceback_filtering()\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "#agent = ActorCritic(0.01, 0.01, 0.9, env.observation_space.shape[0], env.action_space.n, 5)\n",
    "agent = ActorCritic(0.0075, 0.0075, 0.9, env.observation_space.shape[0], env.action_space.n, 5)\n",
    "tf.keras.utils.plot_model(agent.model, show_shapes=True, dpi=70)\n",
    "\n",
    "#agent = ActorCritic(1e-2, 1e-2, 0, 0.9, 4, 2, [128], 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a2f46-b48a-4a85-935b-11f1f52daf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(agent.critic, show_shapes=True, dpi=70)\n",
    "#SVGCircuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a3210-9882-49ea-886e-0e17f8014fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average reward during training episodes and plot.\n",
    "total_rewards = []\n",
    "average_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "max_steps = 500\n",
    "max_episodes = 5000\n",
    "episode_average = 100\n",
    "episode_threshold = 475\n",
    "\n",
    "state_bounds = np.array([2.4, 2.5, 0.21, 2.5])\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  total_reward = 0.0\n",
    "  step = 0\n",
    "  while step != max_steps and not done:\n",
    "    # Calculate action\n",
    "    action = agent.act(observation)\n",
    "\n",
    "    # step forward\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    new_observation = new_observation / state_bounds\n",
    "    angle = observation[2]\n",
    "    position = observation[0]\n",
    "    #reward = 0.05 * (4.8 - np.abs(position)) / 4.8 + 0.95 * (12/np.pi) * (np.pi/12 - np.abs(angle)) \n",
    "    #reward = 0.1 * ((4.8 - np.abs(position))/4.8)**2 + 0.9 * ((np.pi/12 - np.abs(angle))/(np.pi/12))**3 \n",
    "    #reward = 0.05 * ((4.8 - np.abs(position))/4.8)**2 + 0.95 * ((np.pi/12 - np.abs(angle))/(np.pi/12))**2 \n",
    "    reward = 0.25 * (1 - np.abs(position))**3 + 0.75 * (1 - np.abs(angle))**3\n",
    "    agent.learn(observation, action, reward, new_observation, done)\n",
    "\n",
    "    # Update:\n",
    "    total_reward += reward\n",
    "    observation = new_observation\n",
    "    step += 1\n",
    "\n",
    "  agent.memory = []\n",
    "  total_rewards.append(total_reward)\n",
    "  average_rewards.append(total_reward / step)\n",
    "  episode_lengths.append(step)\n",
    "  average_episode_length = np.mean(episode_lengths[-episode_average:])\n",
    "\n",
    "  if len(episode_lengths) > episode_average and average_episode_length > episode_threshold:\n",
    "    print(f\"\\nSolved at Episode {episode}, Average Episode Length: {average_episode_length:.1f}\")\n",
    "    break\n",
    "  else:\n",
    "    print(f\"\\rEpisode: {episode}, Steps: {step: >3}, Average Episode Length: {average_episode_length: >4.1f}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c37d1e-89e3-4193-b4f0-3aeaca365fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = '2024.07.04-17.27'\n",
    "# results = np.load(f'results_{ts}.npy', allow_pickle=True)\n",
    "# total_rewards = results.item().get(\"Total Reward\")\n",
    "# average_rewards = results.item().get(\"Average Reward\")\n",
    "# episode_lengths = results.item().get(\"Episode Length\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "ax[0][0].set_xlabel('Episode')\n",
    "ax[0][0].set_ylabel('Total Reward')\n",
    "ax[0][0].plot(total_rewards)\n",
    "#total_rewards_rolling = np.convolve(total_rewards, np.ones(len(total_rewards)//10), 'same') / (len(total_rewards)//10)\n",
    "# cumsum_vec = np.cumsum(np.insert(total_rewards, 0, 0)) \n",
    "# total_rewards_rolling = (cumsum_vec[len(total_rewards)//10:] - cumsum_vec[:-len(total_rewards)//10 + 1]) / (len(total_rewards)//10)\n",
    "# ax[0][0].plot(total_rewards_rolling)\n",
    "\n",
    "ax[0][1].set_xlabel('Episode')\n",
    "ax[0][1].set_ylabel('Average Reward')\n",
    "ax[0][1].plot(average_rewards)\n",
    "# cumsum_vec = np.cumsum(np.insert(average_rewards, 0, 0)) \n",
    "# average_rewards_rolling = (cumsum_vec[len(average_rewards)//10:] - cumsum_vec[:-len(average_rewards)//10 + 1]) / (len(average_rewards)//10)\n",
    "# ax[0][1].plot(average_rewards_rolling)\n",
    "\n",
    "ax[1][0].set_xlabel('Episode')\n",
    "ax[1][0].set_ylabel('Cumulative Reward')\n",
    "cumulative_rewards = np.cumsum(total_rewards)\n",
    "ax[1][0].plot(cumulative_rewards)\n",
    "\n",
    "ax[1][1].set_xlabel('Episode')\n",
    "ax[1][1].set_ylabel('Episode Length')\n",
    "ax[1][1].plot(episode_lengths, label='Lengths')\n",
    "# cumsum_vec = np.cumsum(np.insert(episode_lengths, 0, 0)) \n",
    "# episode_lengths_rolling = (cumsum_vec[len(episode_lengths)//10:] - cumsum_vec[:-len(episode_lengths)//10 + 1]) / (len(episode_lengths)//10)\n",
    "# ax[1][1].plot(episode_lengths_rolling, 'b--', label='Smoothed Lengths')\n",
    "# ax[1][1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f04eff-2f52-4557-aff4-1ae92b13dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.strftime(\"%Y.%m.%d-%H.%M\")\n",
    "#np.save(f'total_reward_{timestamp}.npy', total_rewards)\n",
    "#np.save(f'running_reward_{timestamp}.npy', running_rewards)\n",
    "#np.save(f'average_reward_{timestamp}.npy', average_rewards)\n",
    "results = {\"Total Reward\":total_rewards,\n",
    "           \"Cumulative Reward\": np.cumsum(total_rewards),\n",
    "           \"Average Reward\": average_rewards,\n",
    "           \"Episode Length\": episode_lengths}\n",
    "np.save(f'qresults_{timestamp}.npy', results)\n",
    "agent.save(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cb8c2-0ced-4903-a59c-b6479441d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play:\n",
    "import matplotlib.animation as anim\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "\n",
    "actor = agent.actor #keras.models.load_model(f\"actor_{timestamp}.keras\", compile=False)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "frames=[]\n",
    "for i in range(500):\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "  observation = observation / state_bounds\n",
    "  action = np.random.choice(env.action_space.n,\n",
    "                                p=actor.predict(observation[np.newaxis, :],\n",
    "                                                 verbose=False).flatten())\n",
    "  observation, _, done, _ = env.step(action)\n",
    "  if done:\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "def init():\n",
    "    global im, text\n",
    "    im = ax.imshow(frames[0])\n",
    "    text = ax.text(.05, .95, '1', ha='left',\n",
    "                   va='top', transform=ax.transAxes) \n",
    "\n",
    "def animate(i):\n",
    "  global im, text\n",
    "  im.set_data(frames[i])\n",
    "  text.set_text(str(i))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.axis('off')\n",
    "\n",
    "animation = anim.FuncAnimation(fig, animate, init_func=init, frames=len(frames))\n",
    "animation.save(f'animation_{timestamp}.gif', writer='ffmpeg', fps=30)\n",
    "plt.close(fig)\n",
    "embed.embed_file(f'animation_{timestamp}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9d915-9750-4470-a170-bba5f0291f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc245e-5383-427e-9c89-ed96eb99353c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb0a5-dd8d-4a11-b9ee-d5db298d01bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68591914-d389-4b0f-a47d-01fdb8d96640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46544c-f1ed-4f63-ab07-219d8f273024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
